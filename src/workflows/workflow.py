from __future__ import annotations

from typing import Optional, Any, Dict, List
from collections import deque

from loguru import logger
from openai import OpenAI
from langchain_core.tools import tool 

from config.settings import settings
from src.retrieval.retriever_rerank import Retriever
from src.generation.rag import RAG
from src.tools.firecrawl_search import FirecrawlSearchTool
from src.indexing.memory_store import MemoryStore


# ===== Prompt æ¨¡æ¿ =====

ROUTER_EVALUATION_TEMPLATE = """ä½ æ˜¯ä¸€ä¸ªæ³•å¾‹ RAG å›ç­”è´¨é‡è¯„ä¼°ä¸“å®¶ï¼Œéœ€è¦ä¸¥è°¨åœ°åˆ¤æ–­ç³»ç»Ÿæ£€ç´¢åˆ°çš„æ³•å¾‹ä¾æ®æ˜¯å¦è¶³ä»¥å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚

ã€ç”¨æˆ·é—®é¢˜ã€‘:
{query}

ã€RAG æ£€ç´¢åˆ°çš„æ³•å¾‹æ¡æ–‡ä¸åˆå§‹å›ç­”ã€‘:
{rag_response}

è¯„ä¼°æ ‡å‡†ï¼š
1. **ç›¸å…³æ€§**ï¼šæ£€ç´¢åˆ°çš„æ³•æ¡æ˜¯å¦ç›´æ¥æ¶µç›–äº†ç”¨æˆ·é—®é¢˜æ¶‰åŠçš„æ³•å¾‹æƒ…å½¢ï¼Ÿ
2. **å‡†ç¡®æ€§**ï¼šå›ç­”æ˜¯å¦æ˜ç¡®å¼•ç”¨äº†å…·ä½“çš„æ³•å¾‹æ³•è§„åç§°å’Œæ¡æ¬¾ç¼–å·ï¼Ÿ
3. **å®Œæ•´æ€§**ï¼šæ˜¯å¦é—æ¼äº†å…³é”®çš„å®šç½ªé‡åˆ‘æ ‡å‡†æˆ–å…è´£æƒ…å½¢ï¼Ÿ
4. **å¹»è§‰æ£€æµ‹**ï¼šå¦‚æœå›ç­”ä¸­å‡ºç°â€œæ ¹æ®ç›¸å…³æ³•å¾‹â€ï¼Œä½†RAGä¸Šä¸‹æ–‡ä¸­æ²¡æœ‰å…·ä½“æ³•æ¡æ”¯æ’‘ï¼Œè§†ä¸ºè´¨é‡ä¸ä½³ã€‚

è¯·ç»™å‡ºä½ çš„æœ€ç»ˆè¯„åˆ¤ï¼ˆäºŒé€‰ä¸€ï¼‰ï¼š
- "GOOD"ï¼šæ£€ç´¢åˆ°çš„æ³•æ¡å……åˆ†ä¸”æ˜ç¡®ï¼Œå¯ä»¥ç›´æ¥åŸºäºæ­¤ç”Ÿæˆä¸“ä¸šæ³•å¾‹æ„è§ã€‚
- "BAD"ï¼šæ£€ç´¢ç»“æœç¼ºå¤±ã€ä¸ç›¸å…³ï¼Œæˆ–ä»…æœ‰ç¬¼ç»Ÿæè¿°è€Œæ— æ³•æ¡æ”¯æ’‘ï¼Œå¿…é¡»è”ç½‘æœç´¢è¡¥å……æœ€æ–°çš„æ³•å¾‹æ³•è§„æˆ–æ¡ˆä¾‹ã€‚

é‡è¦è¦æ±‚ï¼šåªè¿”å›ä¸€ä¸ªå¤§å†™è‹±æ–‡å•è¯ï¼Œä¸è¦åŒ…å«æ ‡ç‚¹ï¼š
- GOOD
- BAD

ä½ çš„è¯„ä¼°ç»“æœï¼š
"""

QUERY_OPTIMIZATION_TEMPLATE = """ä½ æ˜¯ä¸€ä¸ªæ³•å¾‹æ£€ç´¢æŸ¥è¯¢ä¼˜åŒ–ä¸“å®¶ï¼Œéœ€è¦å°†ç”¨æˆ·çš„è‡ªç„¶è¯­è¨€é—®é¢˜è½¬åŒ–ä¸ºé«˜æ•ˆçš„æ³•å¾‹æœç´¢å¼•æ“æŸ¥è¯¢ã€‚

åŸå§‹é—®é¢˜:
{query}

ä¼˜åŒ–åŸåˆ™:
1. **æå–æ³•è¨€æ³•è¯­**ï¼šå°†å£è¯­è½¬åŒ–ä¸ºæ³•å¾‹æœ¯è¯­ï¼ˆå¦‚â€œæ‰“æ¶â€->â€œæ•…æ„ä¼¤å®³â€ã€â€œæ¬ é’±ä¸è¿˜â€->â€œæ°‘é—´å€Ÿè´·çº çº·â€ï¼‰ã€‚
2. **æ˜ç¡®æ³•å¾‹ä¾æ®**ï¼šåŠ å…¥å…³é”®è¯å¦‚â€œä¸­åäººæ°‘å…±å’Œå›½åˆ‘æ³•â€ã€â€œæ°‘æ³•å…¸â€ã€â€œæœ€é«˜æ³•å¸æ³•è§£é‡Šâ€ã€â€œé‡åˆ‘æŒ‡å¯¼æ„è§â€ç­‰ã€‚
3. **æ—¶æ•ˆæ€§**ï¼šå¦‚æœæ¶‰åŠè¿‘æœŸçƒ­ç‚¹æˆ–æ–°è§„ï¼ŒåŠ å…¥â€œ2024â€ã€â€œæœ€æ–°ä¿®è®¢â€ç­‰å…³é”®è¯ã€‚
4. **æ¡ˆä¾‹å¯¼å‘**ï¼šå¦‚æœæ˜¯è¯¢é—®åˆ¤ç½šç»“æœï¼Œå¯ä»¥åŠ å…¥â€œå…¸å‹æ¡ˆä¾‹â€ã€â€œè£åˆ¤æ–‡ä¹¦â€ç­‰è¯ã€‚

ä¼˜åŒ–åçš„æœç´¢æŸ¥è¯¢ï¼ˆä¸­æ–‡ï¼Œä¸€è¡Œï¼Œç²¾ç‚¼ï¼‰ï¼š
"""

SYNTHESIS_TEMPLATE = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ³•å¾‹æ™ºèƒ½åŠ©æ‰‹ï¼ˆParalegal AIï¼‰ï¼Œä½ éœ€è¦åŸºäºã€Œæœ¬åœ°æ³•å¾‹åº“æ£€ç´¢ã€ã€Œè”ç½‘æ³•å¾‹æœç´¢ã€ã€Œé•¿æœŸè®°å¿†ã€ç»¼åˆç”Ÿæˆä¸€ä»½ä¸¥è°¨çš„æ³•å¾‹å’¨è¯¢å›å¤ã€‚

ã€ç”¨æˆ·æ³•å¾‹å’¨è¯¢ã€‘:
{query}

ã€æœ¬åœ°æ³•å¾‹åº“ä¾æ® (RAG)ã€‘:
{rag_response}

ã€è”ç½‘æœç´¢è¡¥å……ä¿¡æ¯ (Web)ã€‘:
{web_results}

ã€å†å²å’¨è¯¢è®°å¿†ã€‘:
{memory_context}

ã€å½“å‰å¯¹è¯ä¸Šä¸‹æ–‡ã€‘:
{dialog_history}

å›ç­”æ’°å†™è¦æ±‚ï¼ˆä¸¥æ ¼éµå®ˆï¼‰ï¼š
1. **æ³•æ¡ä¸ºç‹**ï¼šä¼˜å…ˆä¾æ®æœ¬åœ°æ£€ç´¢åˆ°çš„ç¡®åˆ‡æ³•å¾‹æ¡æ–‡å›ç­”ã€‚å¼•ç”¨æ³•æ¡æ—¶ï¼Œå¿…é¡»ä½¿ç”¨å…¨ç§°ï¼ˆå¦‚ã€Šä¸­åäººæ°‘å…±å’Œå›½åˆ‘æ³•ã€‹ç¬¬äºŒç™¾ä¸‰åå››æ¡ï¼‰ã€‚
2. **åŒºåˆ†æ¥æº**ï¼š
   - å¼•ç”¨æœ¬åœ°åº“å†…å®¹æ—¶ï¼Œè§†ä¸ºâ€œç°æœ‰æ³•å¾‹ä¾æ®â€ï¼›
   - å¼•ç”¨è”ç½‘æœç´¢å†…å®¹æ—¶ï¼Œéœ€æ ‡æ³¨â€œï¼ˆåŸºäºç½‘ç»œæ£€ç´¢ç»“æœï¼‰â€ï¼Œå¹¶æ³¨æ„ç”„åˆ«ä¿¡æ¯çš„æƒå¨æ€§ã€‚
3. **ç»“æ„åŒ–è¾“å‡º**ï¼š
   - **æ ¸å¿ƒç»“è®º**ï¼šç›´æ¥å›ç­”åˆæ³•/è¿æ³•ï¼Œæˆ–å¯èƒ½çš„ç»“æœã€‚
   - **æ³•å¾‹ä¾æ®**ï¼šåˆ—å‡ºå…·ä½“çš„æ³•æ¡åŸæ–‡æˆ–æ¦‚æ‹¬ã€‚
   - **å®åŠ¡å»ºè®®**ï¼šé’ˆå¯¹ç”¨æˆ·æƒ…å†µç»™å‡ºèµ·è¯‰ã€è°ƒè§£æˆ–å–è¯å»ºè®®ã€‚
4. **ä¸¥è°¨å®¢è§‚**ï¼šä¸è¦ä½¿ç”¨â€œè‚¯å®šèƒ½èµ¢â€ã€â€œç™¾åˆ†ä¹‹ç™¾â€ç­‰ç»å¯¹åŒ–è¡¨è¿°ï¼Œä½¿ç”¨â€œå¯èƒ½æ„æˆâ€ã€â€œå­˜åœ¨...é£é™©â€ç­‰ä¸“ä¸šè¡¨è¿°ã€‚
5. **å¼ºåˆ¶å…è´£å£°æ˜**ï¼šå›ç­”çš„æœ€åå¿…é¡»å•ç‹¬ä¸€è¡ŒåŠ ä¸Šï¼š
   *â€œæ³¨ï¼šæœ¬å›å¤ä»…ä¾›å‚è€ƒï¼Œä¸æ„æˆæ­£å¼æ³•å¾‹æ„è§ã€‚å…·ä½“æ¡ˆä»¶è¯·å’¨è¯¢ä¸“ä¸šå¾‹å¸ˆæˆ–ç›¸å…³å¸æ³•éƒ¨é—¨ã€‚â€*

è¯·ç”Ÿæˆæœ€ç»ˆçš„æ³•å¾‹å’¨è¯¢å›å¤ï¼š
"""


REFINE_TEMPLATE = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ³•å¾‹æ™ºèƒ½åŠ©æ‰‹ï¼Œè¯·å¯¹ä¸‹é¢è¿™æ®µåŸºäºæœ¬åœ°æ³•å¾‹åº“çš„å›ç­”è¿›è¡Œæ¶¦è‰²ï¼Œä½¿å…¶æ›´å…·å¾‹å¸ˆé£èŒƒã€‚

ã€åŸå§‹å›ç­”ã€‘:
{rag_response}

ã€å‚è€ƒä¿¡æ¯ã€‘:
{memory_context}
{dialog_history}

æ¶¦è‰²è¦æ±‚ï¼š
1. **å»å£è¯­åŒ–**ï¼šä½¿ç”¨æ³•è¨€æ³•è¯­ï¼ˆå¦‚å°†â€œåç‰¢â€æ”¹ä¸ºâ€œæ‰¿æ‹…åˆ‘äº‹è´£ä»»/æœ‰æœŸå¾’åˆ‘â€ï¼‰ã€‚
2. **é€»è¾‘å¢å¼º**ï¼šä½¿ç”¨â€œé¦–å…ˆã€å…¶æ¬¡ã€ç»¼ä¸Šæ‰€è¿°â€ç­‰è¿æ¥è¯æ¢³ç†é€»è¾‘ã€‚
3. **å¼•ç”¨è§„èŒƒ**ï¼šç¡®ä¿å¼•ç”¨çš„æ³•æ¡æ ¼å¼è§„èŒƒï¼ˆã€Šæ³•å¾‹åç§°ã€‹+ æ¡æ¬¾å·ï¼‰ã€‚
4. **é£é™©æç¤º**ï¼šå¦‚æœæ³•å¾‹è§„å®šæœ‰æ¨¡ç³Šåœ°å¸¦ï¼Œåº”æç¤ºè¯‰è®¼é£é™©ã€‚
5. **å¼ºåˆ¶å…è´£å£°æ˜**ï¼šå›ç­”æœ€åå¿…é¡»åŠ ä¸Šï¼š
   *â€œæ³¨ï¼šæœ¬å›å¤ä»…ä¾›å‚è€ƒï¼Œä¸æ„æˆæ­£å¼æ³•å¾‹æ„è§ã€‚å…·ä½“æ¡ˆä»¶è¯·å’¨è¯¢ä¸“ä¸šå¾‹å¸ˆã€‚â€*

è¯·ç»™å‡ºæ¶¦è‰²åçš„ä¸“ä¸šæ³•å¾‹å›ç­”ï¼š
"""


class PaperAgentWorkflow:
    """
    åŸºäº RAG + OpenAI SDK + LangChain Tools çš„æ³•å¾‹è¾…åŠ©åŠ©æ‰‹ (ParalegalAgent) Workflowï¼š

    æµç¨‹ï¼š
    1. ä½¿ç”¨ RAGï¼ˆæ³•å¾‹æ³•è§„å‘é‡åº“ï¼‰å¾—åˆ°åˆå§‹æ³•æ¡ä¾æ®ï¼›
    2. ç”¨ LLM è¯„ä¼°è¯¥å›ç­”è´¨é‡ï¼ˆGOOD / BADï¼‰ï¼›
    3. å¦‚æœ GOODï¼š
        - åŸºäºæ³•æ¡ç”Ÿæˆä¸“ä¸šæ³•å¾‹æ„è§ï¼Œç»“åˆâ€œé•¿æœŸè®°å¿† + æœ€è¿‘å¯¹è¯â€ï¼›
    4. å¦‚æœ BADï¼š
        - ä¼˜åŒ–ä¸ºæ³•å¾‹ä¸“ä¸šæœç´¢è¯ï¼›
        - è°ƒç”¨ FirecrawlSearchTool æœç´¢æœ€æ–°æ³•å¾‹æ³•è§„æˆ–æ¡ˆä¾‹ï¼›
        - ç»¼åˆ RAG + Web + è®°å¿†ï¼Œç”Ÿæˆæœ€ç»ˆæ³•å¾‹å»ºè®®ï¼›
    5. æ¯è½®ç»“æŸï¼š
        - æ›´æ–°çŸ­æœŸè®°å¿†
        - æ²‰æ·€æœ‰ä»·å€¼çš„æ³•å¾‹å’¨è¯¢è®°å½•åˆ°é•¿æœŸè®°å¿†
    """

    def __init__(
        self,
        retriever: Retriever,
        rag_system: RAG,
        qwen3_api_key: Optional[str] = None,
        qwen3_base_url: Optional[str] = None,
        memory_store: Optional[MemoryStore] = None,
        short_memory_max_turns: int = 6,
        memory_top_k: int = 3,
    ) -> None:
        self.retriever = retriever
        self.rag = rag_system

        # ==== OpenAI SDK å®¢æˆ·ç«¯ï¼ˆç»Ÿä¸€åœ¨è¿™é‡Œé…ç½®ï¼‰====
        api_key = qwen3_api_key or settings.qwen3_api_key
        base_url = qwen3_base_url or settings.qwen3_base_url

        if not api_key:
            raise ValueError("Qwen3 API key æœªé…ç½®ï¼Œè¯·åœ¨ settings.qwen3_api_key æˆ–ç¯å¢ƒå˜é‡ä¸­è®¾ç½®ã€‚")

        self.client = OpenAI(api_key=api_key, base_url=base_url)
        self.llm_model = settings.llm_model

        # Web æœç´¢å·¥å…·ï¼ˆFirecrawl æˆ–ä½ ä¹‹åè‡ªå®šä¹‰çš„æœç´¢å·¥å…·ï¼‰
        self.web = FirecrawlSearchTool()
    
        # é•¿æœŸè®°å¿†
        self.memory_store = memory_store
        self.memory_top_k = memory_top_k

        # çŸ­æœŸè®°å¿†ï¼šæœ€è¿‘å‡ è½®å¯¹è¯ï¼ˆuser/assistantï¼‰
        self.short_memory = deque(maxlen=short_memory_max_turns) 

        logger.info(
            f"[Workflow] PaperAgentWorkflow initialized | "
            f"short_memory_max_turns={short_memory_max_turns}, "
            f"memory_top_k={memory_top_k}, "
            f"memory_enabled={self.memory_store is not None}"
        )

    # ========= å·¥å…· 1ï¼šRAG å›ç­” =========

    # @tool("paper_rag_answer")
    def rag_answer(self, query: str, top_k: Optional[int] = None) -> Dict[str, Any]:
        """
        ä½¿ç”¨ RAG ç³»ç»Ÿï¼ŒåŸºäºæœ¬åœ°æ³•å¾‹åº“æ£€ç´¢ç›¸å…³æ³•æ¡ã€‚
        è¿”å›ç»“æ„ä¸º RAG.get_detailed_response çš„ç»“æœã€‚
        """
        logger.info(f"[RAG] Running detailed RAG for query={query!r}, top_k={top_k}")
        result = self.rag.get_detailed_response(query, top_k=top_k)
        return result

    # ========= å·¥å…· 2ï¼šè¯„ä¼° RAG å›ç­”è´¨é‡ =========

    # @tool("paper_rag_evaluator")
    def evaluate_rag_answer(self, query: str, rag_response: str) -> str:
        """
        ä½¿ç”¨ LLM è¯„ä¼°RAGæ£€ç´¢åˆ°çš„æ³•æ¡æ˜¯å¦è¶³ä»¥å›ç­”é—®é¢˜ï¼Œè¿”å› 'GOOD' æˆ– 'BAD'ã€‚
        """
        prompt = ROUTER_EVALUATION_TEMPLATE.format(query=query, rag_response=rag_response)

        logger.info("[Eval] Evaluating RAG answer quality...")
        resp = self.client.chat.completions.create(
            model=self.llm_model,
            messages=[{"role": "user", "content": prompt}],
            temperature=0.0,
            max_tokens=4,  # ä¸€ä¸ªå•è¯è¶³å¤Ÿ
        )
        text = (resp.choices[0].message.content or "").strip().upper()
        # åªæ‹¿ç¬¬ä¸€ä¸ª tokenï¼Œé˜²å¾¡ä¸€ä¸‹ LLM å¶å°”å•°å—¦çš„æƒ…å†µ
        label = text.split()[0] if text else "BAD"
        if label not in {"GOOD", "BAD"}:
            label = "BAD"

        logger.info(f"[Eval] Evaluation result: {label}")
        return label

    # ========= å·¥å…· 3ï¼šä¼˜åŒ– Web æœç´¢æŸ¥è¯¢ =========

    # @tool("paper_web_query_optimizer")
    def optimize_query_for_web(self, query: str) -> str:
        # è·å–æœ€è¿‘çš„å¯¹è¯å†å²
        history = self._build_dialog_history_block()
            
        # æ„é€ æ›´ä¸°å¯Œçš„ Prompt
        prompt = (
                f"ä½ æ˜¯ä¸€ä¸ªæ³•å¾‹æœç´¢ä¼˜åŒ–ä¸“å®¶ã€‚è¯·æ ¹æ®å¯¹è¯å†å²å’Œç”¨æˆ·é—®é¢˜ï¼Œç”Ÿæˆæœç´¢å…³é”®è¯ã€‚\n\n"
                f"ã€å¯¹è¯å†å²ã€‘:\n{history}\n\n"
                f"ã€ç”¨æˆ·å½“å‰é—®é¢˜ã€‘: {query}\n\n"
                f"è¯·ç”Ÿæˆä¸€ä¸ªå…·ä½“çš„æ³•å¾‹æœç´¢æŸ¥è¯¢ï¼ˆä¾‹å¦‚åŒ…å«å…·ä½“çš„æ³•å¾‹åç§°ï¼‰ï¼š"
        )
        
        logger.info("[Web] Optimizing query for web search...")
        resp = self.client.chat.completions.create(
            model=self.llm_model,
            messages=[{"role": "user", "content": prompt}],
            temperature=0.3,
            max_tokens=128,
        )
        optimized = (resp.choices[0].message.content or "").strip()
        logger.info(f"[Web] Optimized query: {optimized!r}")
        return optimized or query

    # ========= å·¥å…· 4ï¼šæ‰§è¡Œ Web æœç´¢ =========

    # @tool("paper_web_search")
    def web_search(self, optimized_query: str, limit: int = 3) -> str:
        """
        ä½¿ç”¨ FirecrawlSearchTool è¿›è¡Œæ³•å¾‹ç½‘ç»œæœç´¢ï¼Œè¿”å›æ•´ç†åçš„æ–‡æœ¬æ‘˜è¦ã€‚
        """
        logger.info(f"[Web] Running Firecrawl web search, query={optimized_query!r}")
        try:
            results = self.web.invoke({"query": optimized_query, "limit": limit})
            # === ğŸ›‘ è°ƒè¯•æ‰“å° ===
            print(f"\n[DEBUG] Web Search Results Length: {len(results)}")
            print(f"[DEBUG] Web Search Content Preview: {results[:200]}...\n")
            # =================
        except Exception as e:
            logger.error(f"[Web] Web search failed: {e}")
            results = "ç”±äºæŠ€æœ¯é—®é¢˜ï¼Œç½‘ç»œæœç´¢å¤±è´¥."
        return results

# ========= çŸ­æœŸè®°å¿†ç›¸å…³ =========

    def _update_short_memory(self, query: str, answer: str) -> None:
        """è®°å½•æœ€è¿‘å‡ è½®å¯¹è¯"""
        self.short_memory.append(("user", query))
        self.short_memory.append(("assistant", answer))

    def _build_dialog_history_block(self) -> str:
        """æŠŠ short_memory è½¬æˆå­—ç¬¦ä¸²ï¼Œå–‚ç»™ LLM ç”¨"""
        if not self.short_memory:
            return "ï¼ˆæš‚æ— å†å²å¯¹è¯ï¼‰"
        lines: List[str] = []
        for role, content in self.short_memory:
            prefix = "ç”¨æˆ·ï¼š" if role == "user" else "åŠ©æ‰‹ï¼š"
            lines.append(f"{prefix}{content}")
        return "\n".join(lines)

    # ========= é•¿æœŸè®°å¿†ç›¸å…³ =========

    def _search_long_term_memory(self, query: str) -> List[Dict[str, Any]]:
        if self.memory_store is None:
            return []
        results = self.memory_store.search(
            query=query,
            top_k=self.memory_top_k,
            score_threshold=0.35,  # å¯ä»¥æŒ‰éœ€è¦è°ƒ
        )
        return results

    def _build_memory_context_block(self, memories: List[Dict[str, Any]]) -> str:
        if not memories:
            return "ï¼ˆå½“å‰é—®é¢˜å°šæ— ç‰¹åˆ«ç›¸å…³çš„é•¿æœŸè®°å¿†ï¼‰"
        lines = []
        for i, m in enumerate(memories, start=1):
            lines.append(f"[è®°å¿† {i}] {m['text']}")
        return "\n".join(lines)

    def _maybe_write_long_term_memory(self, query: str, answer: str) -> None:
        """
        æ³•å¾‹å’¨è¯¢é€šå¸¸æ¯”è¾ƒå¤æ‚ï¼Œå»ºè®®é€‚å½“æ”¾å®½å†™å…¥é—¨æ§›ï¼Œæˆ–è€…åŸºäºå›ç­”çš„ä¸“ä¸šåº¦æ¥åˆ¤æ–­ã€‚
        è¿™é‡Œä¿æŒ > 50 å­—çš„ç²—æš´ç­–ç•¥ï¼Œä¹Ÿå¯ä»¥åç»­æ”¹ä¸ºâ€œæ˜¯å¦åŒ…å«æ³•æ¡å¼•ç”¨â€æ¥åˆ¤æ–­ä»·å€¼ã€‚
        """
        if self.memory_store is None:
            return

        text = (answer or "").strip()
        if len(text) < 50:
            return

        memory_text = f"Q: {query}\nA: {text}"
        self.memory_store.add_memory(memory_text)

    # ========= å·¥å…· 5ï¼šç»¼åˆ / ç²¾ä¿® æœ€ç»ˆå›ç­” =========

    # @tool("paper_synthesize_answer")
    def synthesize_answer(
        self,
        query: str,
        rag_response: str,
        web_results: Optional[str] = "",
        use_web_results: bool = False,
        top_k: int = 3,
        dialog_history: Optional[list] = None,
        memory_context: Optional[str] = "",
    ) -> str:
        """
        æ ¹æ®æ ‡å¿—å†³å®šï¼š
        - å¦‚æœ use_web_results=Trueï¼šç»¼åˆ RAG å›ç­” + Web æœç´¢ç»“æœ + é•¿æœŸè®°å¿† + å¯¹è¯å†å²ï¼›
        - å¦åˆ™ï¼šå¯¹ RAG å›ç­”è¿›è¡Œä¸­æ–‡æ¶¦è‰²ï¼ŒåŒæ ·å¯ä»¥å‚è€ƒé•¿æœŸè®°å¿†å’Œå¯¹è¯å†å²ã€‚
        """
        citations = self.retriever.get_citations(query=query, top_k=top_k)
        citation_text = ""
        for c in citations:
            snippet = c["snippet"]
            citation_text += f"[{c['rank']}] {snippet}\n\n"

        # 2) ä¿è¯ memory_context / dialog_history æœ‰é»˜è®¤æ–‡æœ¬ï¼Œé¿å…å˜æˆ None
        memory_block = memory_context or "ï¼ˆå½“å‰é—®é¢˜å°šæ— ç‰¹åˆ«ç›¸å…³çš„é•¿æœŸè®°å¿†ï¼‰"
        dialog_block = dialog_history or "ï¼ˆæš‚æ— å†å²å¯¹è¯ï¼‰"

        if use_web_results and web_results:
            logger.info("[Synth] Synthesizing answer from RAG + Web results + Memory + Dialog history...")
            prompt = SYNTHESIS_TEMPLATE.format(
                query=query,
                rag_response=rag_response,
                web_results=web_results,
                memory_context=memory_block,
                dialog_history=dialog_block,
            )
        else:
            logger.info("[Synth] Refining RAG-only answer (with Memory + Dialog history)...")
            prompt = REFINE_TEMPLATE.format(
                rag_response=rag_response,
                memory_context=memory_block,
                dialog_history=dialog_block,
                )

        resp = self.client.chat.completions.create(
            model=self.llm_model,
            messages=[{"role": "user", "content": prompt}],
            temperature=0.3,
            max_tokens=settings.max_tokens,
        
        )
        final_answer = resp.choices[0].message.content or ""

        return final_answer.strip()

    # ========= æ ¸å¿ƒï¼šåŒæ­¥æµç¨‹è°ƒåº¦ =========

    def _run_workflow_sync(self, query: str, top_k: Optional[int] = 3) -> Dict[str, Any]:
        """
        åŒæ­¥ï¼šå®Œæ•´æ‰§è¡Œä¸€æ¬¡æ³•å¾‹å’¨è¯¢å·¥ä½œæµã€‚
        """
        logger.info(f"[Workflow] Starting workflow for query={query!r}, top_k={top_k}")

        # Step 0: å…ˆæŸ¥é•¿æœŸè®°å¿†ï¼ˆä¸ä¼šæŒ¡ä½ RAGï¼Œåªæ˜¯æä¾›é¢å¤– contextï¼‰
        memory_hits = self._search_long_term_memory(query)
        memory_context = self._build_memory_context_block(memory_hits)
        dialog_history = self._build_dialog_history_block()

        # Step 1: æ³•å¾‹åº“ RAG åˆå§‹æ£€ç´¢
        rag_result = self.rag_answer(query=query, top_k=top_k)
        rag_response: str = rag_result.get("response", "")
        sources = rag_result.get("sources", [])

        # Step 2: è¯„ä¼° RAG æ£€ç´¢åˆ°çš„æ³•æ¡æ˜¯å¦å……è¶³
        evaluation = self.evaluate_rag_answer(query=query, rag_response=rag_response)

        if evaluation == "GOOD":
            # RAG æ³•æ¡å……è¶³ â†’ ç”Ÿæˆä¸“ä¸šæ³•å¾‹æ„è§
            final_answer = self.synthesize_answer(
                query=query,
                rag_response=rag_response,
                web_results="",
                use_web_results=False,
                memory_context=memory_context,
                dialog_history=dialog_history,
            )
            web_used = False
            web_results = None
        else:
            # RAG æ³•æ¡ç¼ºå¤± â†’ è”ç½‘æœç´¢æœ€æ–°æ³•å¾‹/æ¡ˆä¾‹
            optimized_query = self.optimize_query_for_web(query=query)
            web_results = self.web_search(optimized_query=optimized_query, limit=3)
            final_answer = self.synthesize_answer(
                query=query,
                rag_response=rag_response,
                web_results=web_results,
                use_web_results=True,
                memory_context=memory_context,
                dialog_history=dialog_history,
            )
            web_used = True

        # Step 3: æ›´æ–° short_memory + å†™å…¥é•¿æœŸè®°å¿†
        self._update_short_memory(query=query, answer=final_answer)
        self._maybe_write_long_term_memory(query=query, answer=final_answer)

        result = {
            "answer": final_answer,
            "rag_response": rag_response,
            "web_search_used": web_used,
            "web_results": web_results,
            "sources": sources,
            "evaluation": evaluation,
            "query": query,
            "memory_hits": memory_hits,
        }

        logger.info("[Workflow] Finished workflow")
        return result

    # ========= å¯¹å¤–ï¼šå¼‚æ­¥æ¥å£ =========

    async def run_workflow(self, query: str, top_k: Optional[int] = 3) -> Dict[str, Any]:
        """
        å¼‚æ­¥ç‰ˆæœ¬ï¼Œä¾¿äºåœ¨ FastAPI / Streamlit ç­‰æ¡†æ¶ä¸­ä½¿ç”¨ã€‚
        å½“å‰å†…éƒ¨æ­¥éª¤æ˜¯åŒæ­¥è°ƒç”¨ï¼Œå¦‚æœä½ åé¢æƒ³å®Œå…¨å¼‚æ­¥åŒ–ï¼Œå¯ä»¥ç”¨ asyncio.to_thread åŒ…ä¸€å±‚ã€‚
        """
        # è¿™é‡Œå…ˆç®€å•ç›´æ¥åŒæ­¥æ‰§è¡Œï¼›éœ€è¦çœŸæ­£éé˜»å¡æ—¶ï¼Œå¯ä»¥æ”¹æˆï¼š
        # return await asyncio.to_thread(self._run_workflow_sync, query, top_k)
        return self._run_workflow_sync(query=query, top_k=top_k)
